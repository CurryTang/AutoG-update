batch_size: 3599
epochs: 500
eval_batch_size: 4096
feat_encode_size: 167
lr: 0.00011428569915189146
nn_config:
  attn_dropout: 0.23315559718963663
  dropout: 0.4710631273407012
  hid_size: 37
  include_first_norm: false
  num_heads: 5
  num_layers: 3
  use_token: false
nn_name: fttransformer
patience: 30
time_budget: 36000
